{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688a2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- Librerías -------------------------------------------------------\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F   # <-- todas las funciones SQL de Spark\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e1b5bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sesión de Spark creada\n",
      "  Network timeout: 800s\n"
     ]
    }
   ],
   "source": [
    "#-----------------------SparkSession---------------------------------------------\n",
    "# Misma configuración que antes\n",
    "# se añadio spark.sql.adaptive.enabled = true para que Catalyst optimice aún más. \n",
    "# Ajusta el plan de ejecución en runtime según estadísticas reales de los datos.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC_Taxi_DataFrame_DeepLearning\") \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    "    .config(\"spark.default.parallelism\", \"16\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\" Sesión de Spark creada\")\n",
    "print(\"  Network timeout: 800s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69cbdabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cargando dataset\n",
      " Numero de registros: 2,964,624\n",
      " Columnas: 19\n"
     ]
    }
   ],
   "source": [
    "#----------------------Cargar datos----------------------------------------------\n",
    "# Igual que antes — spark.read.parquet devuelve un DataFrame directamente.\n",
    "DATA_PATH = \"C:/Users/PC/Documents/DocumentosGustavo/Github/Maestria/BigData/nyc-taxi-spark/data/yellow/2024/yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "print(\"\\n Cargando dataset\")\n",
    "df = spark.read.parquet(DATA_PATH)\n",
    "print(f\" Numero de registros: {df.count():,}\")\n",
    "print(f\" Columnas: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82b0fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Esquema del dataset:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEsquema del dataset:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bbc45f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5774545a",
   "metadata": {},
   "source": [
    "**Antes (RDD):** se usaba una función Python `extract_and_scale_features` aplicada con `.map()`.  \n",
    "Esto serializa cada fila Python ↔ JVM (Java Virtual Machine), lo cual es costoso.\n",
    "\n",
    "**Ahora (DataFrame):** todo el filtrado y la creación de columnas se hace con  \n",
    "`pyspark.sql.functions`, que ejecuta directamente en la JVM de Spark sin cruzar a Python.\n",
    "\n",
    "| RDD | DataFrame |\n",
    "|-----|----------|\n",
    "| `lambda row: row.trip_distance` | `F.col(\"trip_distance\")` |\n",
    "| `if x is None or x <= 0` | `.filter(F.col(...).isNotNull() & ...)` |\n",
    "| `float(datetime.hour)` | `F.hour(F.col(\"tpep_pickup_datetime\"))` |\n",
    "| `(x - μ) / σ` manual | expresión algebraica sobre columnas |\n",
    "| `.filter(lambda x: x is not None)` | `.dropna()` o `.filter(...)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0485d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando features con DataFrame API\n",
      "Registros después de la limpieza: 2,722,784 registros.\n",
      " Completado en 4.4s.\n"
     ]
    }
   ],
   "source": [
    "#----------------------Feature Engineering con DataFrame--------------------------\n",
    "print(\"\\nProcesando features con DataFrame API\")\n",
    "start = time.time()\n",
    "\n",
    "#Seleccionar columnas de interés\n",
    "df_selected = df.select(\n",
    "    \"trip_distance\",\n",
    "    \"passenger_count\",\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"fare_amount\"\n",
    ")\n",
    "\n",
    "#Filtrado (equivalente al if/return None del RDD)\n",
    "#Mismas reglas de negocio que con rdds: distancia, pasajeros y tarifa válidos.\n",
    "df_filtered = df_selected.filter(\n",
    "    F.col(\"trip_distance\").isNotNull() &\n",
    "    F.col(\"passenger_count\").isNotNull() &\n",
    "    F.col(\"tpep_pickup_datetime\").isNotNull() &\n",
    "    F.col(\"fare_amount\").isNotNull() &\n",
    "    (F.col(\"trip_distance\") > 0)  & (F.col(\"trip_distance\") < 100) &\n",
    "    (F.col(\"passenger_count\") > 0) & (F.col(\"passenger_count\") <= 6) &\n",
    "    (F.col(\"fare_amount\") > 0)    & (F.col(\"fare_amount\") < 200)\n",
    ")\n",
    "\n",
    "#Extraer hora y día de la semana directamente con funciones Spark\n",
    "#F.hour() y F.dayofweek() corren en la JVM, sin llamar a Python por fila.\n",
    "#Nota: F.dayofweek() devuelve 1=Domingo ... 7=Sábado (convención Spark/SQL)\n",
    "#Para mantener la misma convención que antes (1=Lunes) ajustamos con ((dow+5)%7)+1\n",
    "df_with_time = df_filtered.withColumn(\n",
    "    \"hour_value\", F.hour(F.col(\"tpep_pickup_datetime\")).cast(DoubleType())\n",
    ").withColumn(\n",
    "    \"day_of_week\",\n",
    "    (((F.dayofweek(F.col(\"tpep_pickup_datetime\")) + 5) % 7) + 1).cast(DoubleType())\n",
    ")\n",
    "\n",
    "#Normalización z-score con los mismos parámetros que el RDD original\n",
    "#trip_distance:   (x - 3.0) / 5.0\n",
    "#passenger_count: (x - 1.5) / 1.0\n",
    "#hour_value:      (x - 12.0) / 7.0\n",
    "#day_of_week:     (x - 4.0) / 2.0\n",
    "df_scaled = df_with_time.select(\n",
    "    ((F.col(\"trip_distance\")   - 3.0)  / 5.0).alias(\"feat_distance\"),\n",
    "    ((F.col(\"passenger_count\") - 1.5)  / 1.0).alias(\"feat_passengers\"),\n",
    "    ((F.col(\"hour_value\")      - 12.0) / 7.0).alias(\"feat_hour\"),\n",
    "    ((F.col(\"day_of_week\")     - 4.0)  / 2.0).alias(\"feat_dow\"),\n",
    "    F.col(\"fare_amount\").cast(DoubleType()).alias(\"label\")\n",
    ")\n",
    "\n",
    "#Cachear el DataFrame procesado (equivalente a .cache() en RDD)\n",
    "df_scaled = df_scaled.repartition(16).cache()\n",
    "\n",
    "total_scaled = df_scaled.count()  # acción que materializa el cache\n",
    "print(f\"Registros después de la limpieza: {total_scaled:,} registros.\")\n",
    "print(f\" Completado en {time.time()-start:.1f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52bbf8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+--------+-----+\n",
      "|       feat_distance|feat_passengers|           feat_hour|feat_dow|label|\n",
      "+--------------------+---------------+--------------------+--------+-----+\n",
      "|              -0.046|           -0.5|  0.8571428571428571|    -1.5| 19.1|\n",
      "|              -0.274|            4.5| -0.5714285714285714|    -1.0| 10.7|\n",
      "|-0.22999999999999998|           -0.5| 0.14285714285714285|     1.0| 11.4|\n",
      "| 0.06200000000000001|           -0.5|  1.2857142857142858|    -0.5| 16.3|\n",
      "|              -0.292|           -0.5|-0.14285714285714285|     1.0|  8.6|\n",
      "+--------------------+---------------+--------------------+--------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# registros limpios con dataframe \n",
    "df_scaled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f033a616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train: 2,177,235 registros\n",
      " Test:  545,549 registros\n"
     ]
    }
   ],
   "source": [
    "#----------------------División Train/Test----------------------------------------\n",
    "# randomSplit funciona igual en DataFrames que en RDDs.\n",
    "train_df, test_df = df_scaled.randomSplit([0.8, 0.2]) #Se quita el seed para que la división sea diferente cada vez, mostrando la variabilidad del proceso.\n",
    "\n",
    "# Persistir en memoria+disco (mismo comportamiento que StorageLevel.MEMORY_AND_DISK)\n",
    "from pyspark import StorageLevel\n",
    "train_df = train_df.repartition(16).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "test_df  = test_df.repartition(8).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count  = test_df.count()\n",
    "\n",
    "print(f\"\\n Train: {train_count:,} registros\")\n",
    "print(f\" Test:  {test_count:,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90859876",
   "metadata": {},
   "source": [
    "Arquitectura de la red neuronal: \n",
    "- capa oculta 1: 64 neuronas, función de activación relu\n",
    "- capa oculta 2: 32 neuronas, función de activación relu\n",
    "- capa oculta 3: 16 neuronas, función de activación relu\n",
    "- capa oculta 4:  8 neuronas, función de activación relu\n",
    "- capa de salida: 1 neurona , función de activación lineal\n",
    "\n",
    "- Optimizador: Adam\n",
    "- Función de costo: MSE\n",
    "- Metricas adicionales: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25ddb059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Modelo creado\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,457</span> (13.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,457\u001b[0m (13.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,265</span> (12.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,265\u001b[0m (12.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----------------------Modelo-----------------------------------------------------\n",
    "# La tasa de aprendizaje es 0.001\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(4,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8,  activation='relu'),\n",
    "        Dense(1,  activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(0.001), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "print(\"\\n Modelo creado\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7327a76",
   "metadata": {},
   "source": [
    "Generador de batches con dataframe: \n",
    "\n",
    "**Antes (RDD):** `toLocalIterator()` iteraba partición por partición, convirtiendo cada fila a Python.  \n",
    "**Ahora (DataFrame):** se usa `df.sample().toPandas()` que:\n",
    "1. El muestreo corre en la JVM.\n",
    "2. La conversión a Pandas/NumPy se hace una sola vez por batch (más eficiente que fila a fila).\n",
    "3. Permite usar directamente `numpy` arrays para Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46be9251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size: 4096\n",
      "  Batches/época train: 400\n",
      "  Samples/época: 1,638,400\n"
     ]
    }
   ],
   "source": [
    "#----------------------Generador de batches con DataFrame-------------------------\n",
    "class DataFrameBatchGenerator:\n",
    "    \"\"\"\n",
    "    Genera batches para Keras desde un Spark DataFrame.\n",
    "    \n",
    "    Idea: df.sample() + toPandas() por batch.\n",
    "    - sample() corre en JVM: distribuido y eficiente.\n",
    "    - toPandas() trae solo el batch muestreado al driver.\n",
    "    - Sin serialización fila a fila (a diferencia de toLocalIterator en RDD).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, batch_size=4096, num_batches_per_epoch=None,\n",
    "                 feature_cols=None, label_col=\"label\"):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.label_col = label_col\n",
    "        self.feature_cols = feature_cols or [c for c in df.columns if c != label_col]\n",
    "        self.total_samples = df.count()\n",
    "\n",
    "        if num_batches_per_epoch:\n",
    "            self.num_batches = num_batches_per_epoch\n",
    "        else:\n",
    "            self.num_batches = max(1, self.total_samples // batch_size)\n",
    "\n",
    "    def generate_batches(self, seed=42):\n",
    "        \"\"\"\n",
    "        Genera batches usando sample() + toPandas().\n",
    "        \n",
    "        Para obtener múltiples batches independientes se re-muestrea\n",
    "        en cada llamada con una seed distinta, garantizando variedad\n",
    "        sin necesidad de iterar sobre todo el DataFrame.\n",
    "        \"\"\"\n",
    "        samples_needed = self.batch_size * self.num_batches\n",
    "        fraction = min(1.0, samples_needed / self.total_samples)\n",
    "\n",
    "        # sample() ejecuta en JVM — distribuido\n",
    "        sampled_df = self.df.sample(withReplacement=False, fraction=fraction, seed=seed)\n",
    "\n",
    "        # toPandas() trae los datos al driver en un solo batch de red\n",
    "        pandas_df = sampled_df.select(self.feature_cols + [self.label_col]).toPandas()\n",
    "\n",
    "        X_all = pandas_df[self.feature_cols].values.astype(np.float32)\n",
    "        y_all = pandas_df[self.label_col].values.astype(np.float32)\n",
    "\n",
    "        # Yield batch a batch desde el array en memoria\n",
    "        for i in range(0, len(X_all), self.batch_size):\n",
    "            X_batch = X_all[i : i + self.batch_size]\n",
    "            y_batch = y_all[i : i + self.batch_size]\n",
    "            if len(X_batch) == 0:\n",
    "                break\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "\n",
    "# Configuración — mismos hiperparámetros que la versión RDD\n",
    "BATCH_SIZE             = 4096\n",
    "BATCHES_PER_EPOCH_TRAIN = 400\n",
    "BATCHES_PER_EPOCH_VAL   = 20\n",
    "\n",
    "FEATURE_COLS = [\"feat_distance\", \"feat_passengers\", \"feat_hour\", \"feat_dow\"]\n",
    "\n",
    "train_generator = DataFrameBatchGenerator(\n",
    "    train_df,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches_per_epoch=BATCHES_PER_EPOCH_TRAIN,\n",
    "    feature_cols=FEATURE_COLS\n",
    ")\n",
    "\n",
    "test_generator = DataFrameBatchGenerator(\n",
    "    test_df,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_batches_per_epoch=BATCHES_PER_EPOCH_VAL,\n",
    "    feature_cols=FEATURE_COLS\n",
    ")\n",
    "\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Batches/época train: {BATCHES_PER_EPOCH_TRAIN}\")\n",
    "print(f\"  Samples/época: {BATCH_SIZE * BATCHES_PER_EPOCH_TRAIN:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6302d48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento.\n",
      "\n",
      "   Configuración:\n",
      "   Épocas: 15\n",
      "   Batches/época: 400\n",
      "   Batch size: 4096\n",
      "\n",
      "  Iniciando el entrenamiento\n",
      "\n",
      "\n",
      "Época 1/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 448.2045\n",
      "  Batch 200/400 - loss: 254.0927\n",
      "  Batch 300/400 - loss: 179.6310\n",
      "  Batch 400/400 - loss: 142.6634\n",
      "\n",
      "    Época 1:\n",
      "     loss: 303.2677 - mae: 11.5501\n",
      "     val_loss: 137.1804 - val_mae: 6.8106\n",
      "     Tiempo: 12.2s\n",
      "\n",
      "Época 2/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 117.0829\n",
      "  Batch 200/400 - loss: 103.5555\n",
      "  Batch 300/400 - loss: 93.6439\n",
      "  Batch 400/400 - loss: 86.1382\n",
      "\n",
      "    Época 2:\n",
      "     loss: 104.8514 - mae: 5.6784\n",
      "     val_loss: 84.7104 - val_mae: 4.9617\n",
      "     Tiempo: 10.8s\n",
      "\n",
      "Época 3/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 79.0901\n",
      "  Batch 200/400 - loss: 74.6411\n",
      "  Batch 300/400 - loss: 70.8400\n",
      "  Batch 400/400 - loss: 67.6158\n",
      "\n",
      "    Época 3:\n",
      "     loss: 74.6835 - mae: 4.5934\n",
      "     val_loss: 66.9645 - val_mae: 4.3090\n",
      "     Tiempo: 11.2s\n",
      "\n",
      "Época 4/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 64.2617\n",
      "  Batch 200/400 - loss: 61.9809\n",
      "  Batch 300/400 - loss: 59.9413\n",
      "  Batch 400/400 - loss: 58.1441\n",
      "\n",
      "    Época 4:\n",
      "     loss: 61.9409 - mae: 4.1201\n",
      "     val_loss: 57.7809 - val_mae: 3.9617\n",
      "     Tiempo: 11.3s\n",
      "\n",
      "Época 5/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 56.1733\n",
      "  Batch 200/400 - loss: 54.7746\n",
      "  Batch 300/400 - loss: 53.4615\n",
      "  Batch 400/400 - loss: 52.2576\n",
      "\n",
      "    Época 5:\n",
      "     loss: 54.7070 - mae: 3.8420\n",
      "     val_loss: 52.0063 - val_mae: 3.7351\n",
      "     Tiempo: 16.3s\n",
      "\n",
      "Época 6/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 50.9441\n",
      "  Batch 200/400 - loss: 49.9887\n",
      "  Batch 300/400 - loss: 49.0730\n",
      "  Batch 400/400 - loss: 48.2237\n",
      "\n",
      "    Época 6:\n",
      "     loss: 49.9284 - mae: 3.6477\n",
      "     val_loss: 48.0598 - val_mae: 3.5681\n",
      "     Tiempo: 12.0s\n",
      "\n",
      "Época 7/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 47.2758\n",
      "  Batch 200/400 - loss: 46.5662\n",
      "  Batch 300/400 - loss: 45.8721\n",
      "  Batch 400/400 - loss: 45.2109\n",
      "\n",
      "    Época 7:\n",
      "     loss: 46.5145 - mae: 3.4998\n",
      "     val_loss: 45.0904 - val_mae: 3.4362\n",
      "     Tiempo: 11.1s\n",
      "\n",
      "Época 8/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 44.4986\n",
      "  Batch 200/400 - loss: 43.9517\n",
      "  Batch 300/400 - loss: 43.4139\n",
      "  Batch 400/400 - loss: 42.8985\n",
      "\n",
      "    Época 8:\n",
      "     loss: 43.9094 - mae: 3.3817\n",
      "     val_loss: 42.8074 - val_mae: 3.3305\n",
      "     Tiempo: 10.4s\n",
      "\n",
      "Época 9/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 42.3498\n",
      "  Batch 200/400 - loss: 41.9232\n",
      "  Batch 300/400 - loss: 41.4928\n",
      "  Batch 400/400 - loss: 41.0784\n",
      "\n",
      "    Época 9:\n",
      "     loss: 41.8848 - mae: 3.2863\n",
      "     val_loss: 41.0072 - val_mae: 3.2446\n",
      "     Tiempo: 11.7s\n",
      "\n",
      "Época 10/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 40.6256\n",
      "  Batch 200/400 - loss: 40.2701\n",
      "  Batch 300/400 - loss: 39.9099\n",
      "  Batch 400/400 - loss: 39.5592\n",
      "\n",
      "    Época 10:\n",
      "     loss: 40.2361 - mae: 3.2074\n",
      "     val_loss: 39.5028 - val_mae: 3.1724\n",
      "     Tiempo: 10.9s\n",
      "\n",
      "Época 11/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 39.1991\n",
      "  Batch 200/400 - loss: 38.9023\n",
      "  Batch 300/400 - loss: 38.6055\n",
      "  Batch 400/400 - loss: 38.3157\n",
      "\n",
      "    Época 11:\n",
      "     loss: 38.8749 - mae: 3.1416\n",
      "     val_loss: 38.2630 - val_mae: 3.1122\n",
      "     Tiempo: 11.1s\n",
      "\n",
      "Época 12/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 38.0060\n",
      "  Batch 200/400 - loss: 37.7574\n",
      "  Batch 300/400 - loss: 37.4959\n",
      "  Batch 400/400 - loss: 37.2512\n",
      "\n",
      "    Época 12:\n",
      "     loss: 37.7282 - mae: 3.0856\n",
      "     val_loss: 37.2013 - val_mae: 3.0601\n",
      "     Tiempo: 11.0s\n",
      "\n",
      "Época 13/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 36.9761\n",
      "  Batch 200/400 - loss: 36.7599\n",
      "  Batch 300/400 - loss: 36.5362\n",
      "  Batch 400/400 - loss: 36.3189\n",
      "\n",
      "    Época 13:\n",
      "     loss: 36.7355 - mae: 3.0373\n",
      "     val_loss: 36.2844 - val_mae: 3.0152\n",
      "     Tiempo: 10.5s\n",
      "\n",
      "Época 14/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 36.0946\n",
      "  Batch 200/400 - loss: 35.9112\n",
      "  Batch 300/400 - loss: 35.7176\n",
      "  Batch 400/400 - loss: 35.5230\n",
      "\n",
      "    Época 14:\n",
      "     loss: 35.8889 - mae: 2.9954\n",
      "     val_loss: 35.4958 - val_mae: 2.9761\n",
      "     Tiempo: 11.0s\n",
      "\n",
      "Época 15/15\n",
      "------------------------------------------------------------\n",
      "  Batch 100/400 - loss: 35.3347\n",
      "  Batch 200/400 - loss: 35.1705\n",
      "  Batch 300/400 - loss: 34.9968\n",
      "  Batch 400/400 - loss: 34.8330\n",
      "\n",
      "    Época 15:\n",
      "     loss: 35.1508 - mae: 2.9586\n",
      "     val_loss: 34.8087 - val_mae: 2.9416\n",
      "     Tiempo: 10.5s\n",
      "\n",
      "================================================================================\n",
      "  Entrenamiento completo\n",
      "================================================================================\n",
      "  Tiempo: 2.87 minutos\n",
      "  Épocas: 15\n",
      "  Mejor val_loss: 34.8087\n"
     ]
    }
   ],
   "source": [
    "#----------------------Entrenamiento----------------------------------------------\n",
    "print(\"Entrenamiento.\")\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "print(f\"\\n   Configuración:\")\n",
    "print(f\"   Épocas: {EPOCHS}\")\n",
    "print(f\"   Batches/época: {BATCHES_PER_EPOCH_TRAIN}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "history = {'loss': [], 'mae': [], 'val_loss': [], 'val_mae': []}\n",
    "\n",
    "print(\"\\n  Iniciando el entrenamiento\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nÉpoca {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    epoch_losses, epoch_maes = [], []\n",
    "    batch_count = 0\n",
    "\n",
    "    try:\n",
    "        for X_batch, y_batch in train_generator.generate_batches(seed=epoch):\n",
    "            metrics = model.train_on_batch(X_batch, y_batch, return_dict=True)\n",
    "            epoch_losses.append(metrics['loss'])\n",
    "            epoch_maes.append(metrics['mae'])\n",
    "            batch_count += 1\n",
    "\n",
    "            if batch_count % 100 == 0:\n",
    "                print(f\"  Batch {batch_count}/{BATCHES_PER_EPOCH_TRAIN} - \"\n",
    "                      f\"loss: {np.mean(epoch_losses[-20:]):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error en batch {batch_count}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if not epoch_losses:\n",
    "        print(\"  No se completaron batches, saltando época\")\n",
    "        continue\n",
    "\n",
    "    train_loss = np.mean(epoch_losses)\n",
    "    train_mae  = np.mean(epoch_maes)\n",
    "\n",
    "    # Validación\n",
    "    val_losses, val_maes = [], []\n",
    "    for X_val, y_val in test_generator.generate_batches(seed=epoch):\n",
    "        val_metrics = model.test_on_batch(X_val, y_val, return_dict=True)\n",
    "        val_losses.append(val_metrics['loss'])\n",
    "        val_maes.append(val_metrics['mae'])\n",
    "\n",
    "    val_loss = np.mean(val_losses) if val_losses else train_loss\n",
    "    val_mae  = np.mean(val_maes)   if val_maes  else train_mae\n",
    "\n",
    "    history['loss'].append(train_loss)\n",
    "    history['mae'].append(train_mae)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mae'].append(val_mae)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"\\n    Época {epoch+1}:\")\n",
    "    print(f\"     loss: {train_loss:.4f} - mae: {train_mae:.4f}\")\n",
    "    print(f\"     val_loss: {val_loss:.4f} - val_mae: {val_mae:.4f}\")\n",
    "    print(f\"     Tiempo: {epoch_time:.1f}s\")\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch > 3 and val_loss > history['val_loss'][-2]:\n",
    "        patience = getattr(model, 'patience', 0) + 1\n",
    "        model.patience = patience\n",
    "        if patience >= 3:\n",
    "            print(f\"\\n   Early stopping\")\n",
    "            break\n",
    "    else:\n",
    "        model.patience = 0\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  Entrenamiento completo\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Tiempo: {training_time/60:.2f} minutos\")\n",
    "print(f\"  Épocas: {len(history['loss'])}\")\n",
    "print(f\"  Mejor val_loss: {min(history['val_loss']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ed8faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando modelo...\n",
      "Resultados\n",
      "\n",
      "  R²:   0.8911 (89.1%)\n",
      "  RMSE: $5.4791\n",
      "  MAE:  $2.5810\n",
      "  MAPE: 17.72%\n",
      "\n",
      "  Evaluado en 409,431 predicciones\n"
     ]
    }
   ],
   "source": [
    "#----------------------Evaluación-------------------------------------------------\n",
    "print(\"\\nEvaluando modelo...\")\n",
    "\n",
    "eval_generator = DataFrameBatchGenerator(\n",
    "    test_df,\n",
    "    batch_size=4096,\n",
    "    num_batches_per_epoch=100,\n",
    "    feature_cols=FEATURE_COLS\n",
    ")\n",
    "\n",
    "all_predictions, all_actuals = [], []\n",
    "\n",
    "for X_test, y_test_batch in eval_generator.generate_batches(seed=99):\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    all_predictions.extend(y_pred.flatten().tolist())\n",
    "    all_actuals.extend(y_test_batch.tolist())\n",
    "\n",
    "y_test_eval = np.array(all_actuals)\n",
    "y_pred_eval = np.array(all_predictions)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "mse  = mean_squared_error(y_test_eval, y_pred_eval)\n",
    "rmse = np.sqrt(mse)\n",
    "mae  = mean_absolute_error(y_test_eval, y_pred_eval)\n",
    "r2   = r2_score(y_test_eval, y_pred_eval)\n",
    "mape = np.mean(np.abs((y_test_eval - y_pred_eval) / y_test_eval)) * 100\n",
    "\n",
    "print(\"Resultados\")\n",
    "print(f\"\\n  R²:   {r2:.4f} ({r2*100:.1f}%)\")\n",
    "print(f\"  RMSE: ${rmse:.4f}\")\n",
    "print(f\"  MAE:  ${mae:.4f}\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "print(f\"\\n  Evaluado en {len(y_test_eval):,} predicciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "376fdf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Guardar----------------------------------------------------\n",
    "#import os\n",
    "#os.makedirs(\"modelos\", exist_ok=True)\n",
    "#model_path = f\"modelos/taxi_DataFrame_{datetime.now().strftime('%Y%m%d_%H%M%S')}.h5\"\n",
    "#model.save(model_path)\n",
    "#print(f\"\\n✓ Modelo guardado: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc114d",
   "metadata": {},
   "source": [
    "Cambios: \n",
    "\n",
    "| Paso | Versión RDD | Versión DataFrame |\n",
    "|------|-------------|------------------|\n",
    "| Selección de columnas | `.select(...).rdd.map(lambda row: ...)` | `.select(\"col1\", \"col2\", ...)` |\n",
    "| Filtrado / limpieza | `lambda row: if ... return None` | `.filter(F.col(...).isNotNull() & ...)` |\n",
    "| Extracción de hora | `datetime.hour` en Python | `F.hour(F.col(\"tpep_pickup_datetime\"))` en JVM |\n",
    "| Día de la semana | `datetime.weekday()` en Python | `F.dayofweek(...)` en JVM |\n",
    "| Normalización | Operaciones Python por fila | Expresiones algebraicas sobre columnas (JVM) |\n",
    "| Cache | `.cache()` sobre RDD | `.cache()` sobre DataFrame (mismo API) |\n",
    "| Persistencia | `StorageLevel.MEMORY_AND_DISK` | `StorageLevel.MEMORY_AND_DISK` (igual) |\n",
    "| Generador de batches | `toLocalIterator()` fila a fila | `sample() + toPandas()` por batch |\n",
    "| Entrenamiento Keras | `train_on_batch()` igual | `train_on_batch()` igual |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
